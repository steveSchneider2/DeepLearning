# -*- coding: utf-8 -*-
"""TF2.0 ANN MNIST.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/161SaEMssCa8vQUZQjIsIynjVqBsKzG7P

The MNIST database is a large database of handwritten digits that is commonly 
used for training various image processing systems. It was created by 
"re-mixing" the samples from NIST's original datasets. 
Furthermore, the black and white images from NIST were normalized to fit into 
a 28x28 pixel bounding box and anti-aliased, which introduced grayscale levels. 
The MNIST database contains 60,000 training images and 10,000 testing images.

This is a Feedforward neural network (the most basic form of ANN)

12 Jan: not fully functional at line 133...index out of range...
12 Jan: It IS fully functional when run in Python38 environment!!!
01 Feb: Now also w/o error in Python 3.7  CondaEnvr: p37tf21cu7

UDEMY TENSORFLOW 2.0 LECTURE 28  MNIST IMAGE CLASSIFICATION 60,000 RECORDS
RUNNING ON GPU: TAKES 36.8 SECONDS
"""

# Commented out IPython magic to ensure Python compatibility.
# Install TensorFlow
# !pip install -q tensorflow-gpu==2.0.0-beta1

# try:
# #   %tensorflow_version 2.x  # Colab only.
# except Exception:
#   pass
#%% Imports
import tensorflow as tf
import time
import sys
import GPUtil
import os

#%% Initialization & Load
gpus = GPUtil.getGPUs()

print(tf.__version__)

time.strftime('%c')
starttime = time.perf_counter()

# Load in the data (internal dataset)
mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0  #  scale values down to 0-1
print("x_train.shape:", x_train.shape)

#%%  Build the model
model = tf.keras.models.Sequential([
  # given image size (28 x 28)  'FLATTEN' to 2 Dimensional n by d input ie n by 784 (like a csv)
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  # 128 hidden units... chosen below by trial & error by the user: 64 worked just as well!
  tf.keras.layers.Dense(128, activation='relu'),
  # randomly drop input nodes...to force NN to learn more evenly from all inputs; done RANDOMLY!!
  tf.keras.layers.Dropout(0.2),  # 20% chance of dropping a node in a layer
  # 10 classes because there are 10 digets... (10 "output classes")
  tf.keras.layers.Dense(10, activation='softmax')])

#%% Compile the model
model.compile(optimizer='adam',
              # eh?
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

#%% Train the model
epochs = 10
r = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=epochs)

endtime = time.perf_counter()
duration = endtime - starttime

modelstart = time.strftime('%c')
print('Modeling time: ', duration)
#%% Graph the model
import pydot
import graphviz
import matplotlib.pyplot as plt
from tensorflow import keras
# from tensorflow.keras import layers
keras.utils.plot_model(model, to_file='mdl.png', show_shapes=True, expand_nested=True)
image=plt.imread('mdl.png')
plt.imshow(image)

#%% Show the model in text
# this is really better, more compact than the graph above
#  At least for simple models.  VGG16 had multiple arrows from one layer...
summary = model.summary()

#%% Chart multi-color Modelling results
import io

s = io.StringIO()
model.summary(print_fn=lambda x: s.write(x + '\n'))
model_summary = s.getvalue()
model_summary = model_summary.replace('=','')
model_summary = model_summary.replace('_','')
model_summary = model_summary.replace('\n\n','\n')

s.close()

# print("The model summary is:\n\n{}".format(model_summary))
# Plot loss per iteration
pver = str(format(sys.version_info.major) + '.' +
           format(sys.version_info.minor) + '.' +
           format(sys.version_info.micro))

condaenv = os.environ['CONDA_DEFAULT_ENV']
plt.style.use('classic')
plt.plot(r.history['loss'], label='loss', linewidth=3)
plt.plot(r.history['val_loss'], label='val_loss', linewidth=3)
# plt.legend()
# Plot accuracy per iteration
plt.plot(r.history['accuracy'], label='acc', linewidth=3)
plt.plot(r.history['val_accuracy'], label='val_acc', linewidth=3)
plt.legend()
plt.title(f'Udemy TensorFlow 2.0 Lecture 28     {modelstart}' +
          '\nArtificial Neural Network: Image Classification' +
          f' {x_train.shape[0]} records')
plt.xlabel('ANN_ImageClassification.py (MNIST Pictures) Duration: ' +
           f' {duration: 3.2f}  Pictures {x_train.shape[1]} by {x_train.shape[2]}')

s2 = """model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dropout(0.2),  
  tf.keras.layers.Dense(10, activation='softmax')])"""

plt.text(.5, .75,# transform=trans1,
         s=s2,
         wrap=True, ha='left', va='bottom',
         fontsize=10, bbox=dict(facecolor='yellow', alpha=0.5))
plt.text(.5, .4,# transform=trans1,
         s=model_summary,
         wrap=True, ha='left', va='bottom', fontname='Consolas',
         fontsize=8, bbox=dict(facecolor='pink', alpha=0.5))
plt.text(1, .15,# transform=trans1,
         s='Artificial neural network (feed forward)' + 
         '\nUsing Keras;' +
         f'{epochs} epochs, Duration: {duration:3.2f} seconds' +
         f'\n{gpus[0].name  } Cuda 11.1.relgpu' +
         f' GPU Memory: {gpus[0].memoryTotal} Mb',
         wrap=True, ha='left', va='bottom',
         fontsize=12, bbox=dict(facecolor='aqua', alpha=0.5))
plt.text(6.5, .7,# transform=trans1,
         s=f'Conda Envr:  {condaenv}\n' +
         f'Gpu  Support:       {tf.test.is_built_with_gpu_support()}\n' +
         f'Cuda Support:       {tf.test.is_built_with_cuda()}\n' +
         f'Tensor Flow:        {tf.version.VERSION}\n'+
         f'Python:             {pver}',
         wrap=True, ha='left', va='top',
         fontsize=9, bbox=dict(facecolor='pink', alpha=0.5))
plt.show()

#%%  Evaluate the model
print(model.evaluate(x_test, y_test))
#  model.evaluate(x_test)  This line not in original code from Lazy programmer

#%% Plot confusion matrix
from sklearn.metrics import confusion_matrix
import numpy as np
import itertools

def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
  """
  This function prints and plots the confusion matrix.
  Normalization can be applied by setting `normalize=True`.
  """
  if normalize:
      cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
      print("Normalized confusion matrix")
  else:
      print('Confusion matrix, without normalization')

  print(cm)

  plt.imshow(cm, interpolation='nearest', cmap=cmap)
  plt.title(title)
  plt.colorbar()
  tick_marks = np.arange(len(classes))
  plt.xticks(tick_marks, classes, rotation=45)
  plt.yticks(tick_marks, classes)

  fmt = '.2f' if normalize else 'd'
  thresh = cm.max() / 2.
  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
      plt.text(j, i, format(cm[i, j], fmt),
               horizontalalignment="center",
               color="white" if cm[i, j] > thresh else "black")

  plt.tight_layout()
  plt.ylabel('True label')
  plt.xlabel('Predicted label')
  plt.show()


p_test = model.predict(x_test).argmax(axis=1)
cm = confusion_matrix(y_test, p_test)
plot_confusion_matrix(cm, list(range(10)))

#%% Real-world validate results

# Do these results make sense?
# It's easy to confuse 9 <--> 4, 9 <--> 7, 2 <--> 7, etc.

# Show some misclassified examples
misclassified_idx = np.where(p_test != y_test)[0]
i = np.random.choice(misclassified_idx)
plt.imshow(x_test[i], cmap='gray')
plt.title("True label: %s Predicted: %s" % (y_test[i], p_test[i]))
plt.xlabel(f'Image# {i}');